---
output: word_document
---
```{r}
# Now we can model the data
#loadrequiredlibrariesforpreprocessing(TRUE)
setwd('/Users/dsing001/LV/R')
library(ggplot2)
library(discretization)
library(randomForest)
library(ROCR)
library(LV)
library(unbalanced)
library(StatMatch)

library(AppliedPredictiveModeling)
library(caret)

clscol='Class'

# read the data which we saved as part of part 2 aka feature selection
train_data <- read.csv('fs_train_data.csv')
test_data <- read.csv('fs_test_data.csv')
valid_data <- read.csv('fs_validdata.csv')


train_data$No..Of.Credit.Lines <- NULL
test_data$No..Of.Credit.Lines <- NULL
valid_data$No..Of.Credit.Lines <- NULL

train_data[,clscol] <- as.factor(as.numeric(train_data[,clscol] ))
test_data[,clscol] <- as.factor(as.numeric(test_data[,clscol] ))
valid_data[,clscol] <- as.factor(as.numeric(valid_data[,clscol] ))
#just put the data for uncreditworthy
td_u <- train_data[train_data[,clscol]==1,]
td_c <- train_data[train_data[,clscol]==0,]

numfolds <- floor(nrow(td_c)/nrow(td_u))

flds <- createFolds(seq(nrow(td_c)),numfolds)

for (i in seq(numfolds))
{
  print (paste('fold',i))
  print 
 td <- train_data[flds[[i]],]
 td <- rbind(td,td_u)
 print(head(td))


#Now add the interaction terms
# from decision tree

frml1 <- 'Class ~ . + Loan.Purpose:FICO.Credit.Score + Loan.Purpose:Use.Of.Credit.Line + Loan.Term:FICO.Credit.Score + Loan.Term:Annual.Income + Loan.Term:Use.Of.Credit.Line +  Annual.Income:FICO.Credit.Score +  Loan.Term:Loan.Purpose + FICO.Credit.Score:No..Inquiries.In.Last.6.Months + No..Inquiries.In.Last.6.Months:Use.Of.Credit.Line +   FICO.Credit.Score:Total.Number.Of.Credit.Lines + Loan.Term:No..Inquiries.In.Last.6.Months'

md_prms <- train_and_predict_log_reg_and_ret_auc(frml1,td,valid_data,predict_type='response')
auc <- md_prms$auc
mdl <- md_prms$model
tst_with_prob <- md_prms$tst_with_prob
#
AUC <- auc$AUC
GC <- (2*AUC) - 1
KS <- auc$KS
KSRealized <- auc$KSRealized
print(paste('AUC',AUC))
print(paste('GC',GC))
print(paste('KS',KS))
rocperf <- auc$rocperf

plot(rocperf,col='blue',xlim = c(0,1), ylim = c(0,1)) 

#Plot the roc curve
# 1- specificity
fpr <- as.vector(attr(rocperf,'x.values')[[1]])
tpr <- as.vector(attr(rocperf,'y.values')[[1]])
pr <- data.frame(cbind(fpr,tpr))
colnames(pr) <- c('fpr','tpr')



cutoffvalues <- as.vector(attr(rocperf,'alpha.values')[[1]])
cutoffvalue <- cutoffvalues[KSRealized]
#cutoffvalue <- 0.5
print(paste('cutoffvalue',cutoffvalue))
#generate the confusion matrix. To get the cutoff I will use the fact that we have KS score. Thus, where we have that value occuring we will have the best accuracy/recall/precision
tst_with_prob$predclass <- ifelse(tst_with_prob$predprob>cutoffvalue,1,0)
#missclassification each one example say where class 1 was predicted as class 0 and vice versa

tst_with_prob_1 <- tst_with_prob[tst_with_prob$Class==1 & tst_with_prob$predclass==0,]
dim(tst_with_prob_1)
tst_with_prob_0 <- tst_with_prob[tst_with_prob$Class==0 & tst_with_prob$predclass==1,]
dim(tst_with_prob_0)
tst_with_prob_1 <- tst_with_prob_1[1,]
tst_with_prob_0 <- tst_with_prob_0[1,]

#find all the data which is similar and show that
k=5
simitms_1 <- order(gower.dist(tst_with_prob_1,tst_with_prob))[seq(1:k)]
#show 5 nearst point to the misclassified example
print(tst_with_prob[simitms_1,])

simitms_0 <- order(gower.dist(tst_with_prob_0,tst_with_prob))[seq(1:k)]
#show 5 nearst point to the misclassified example
print(tst_with_prob[simitms_0,])

#calculate accurayce and recall
ab <- table(tst_with_prob$predclass,tst_with_prob$Class)
print(ab)
recall <- ab[2,2]/(ab[1,2] + ab[2,2])
print(paste('recall',recall))
acc <- (ab[2,2] + ab[1,1])/(sum(ab))
print(paste('accuracy',acc))

#part below this should be run only when you are fine with your model on validation data and you should not cheat by running below an calibrating your model :)
print
print("now check on test daata")
#on test data accuracy
td <- rbind(td,valid_data)
md_prms <- train_and_predict_log_reg_and_ret_auc(frml1,td,valid_data,predict_type='response')
auc <- md_prms$auc
mdl <- md_prms$model
tst_with_prob <- md_prms$tst_with_prob
#
AUC <- auc$AUC
GC <- (2*AUC) - 1
KS <- auc$KS
KSRealized <- auc$KSRealized
print(paste('AUC',AUC))
print(paste('GC',GC))
print(paste('KS',KS))
rocperf <- auc$rocperf

plot(rocperf,col='blue',xlim = c(0,1), ylim = c(0,1)) 

#Plot the roc curve
# 1- specificity
fpr <- as.vector(attr(rocperf,'x.values')[[1]])
tpr <- as.vector(attr(rocperf,'y.values')[[1]])
pr <- data.frame(cbind(fpr,tpr))
colnames(pr) <- c('fpr','tpr')



cutoffvalues <- as.vector(attr(rocperf,'alpha.values')[[1]])
cutoffvalue <- cutoffvalues[KSRealized]
#cutoffvalue <- 0.5
print(paste('cutoffvalue',cutoffvalue))
#generate the confusion matrix. To get the cutoff I will use the fact that we have KS score. Thus, where we have that value occuring we will have the best accuracy/recall/precision
tst_with_prob$predclass <- ifelse(tst_with_prob$predprob>cutoffvalue,1,0)
#missclassification each one example say where class 1 was predicted as class 0 and vice versa

tst_with_prob_1 <- tst_with_prob[tst_with_prob$Class==1 & tst_with_prob$predclass==0,]
dim(tst_with_prob_1)
tst_with_prob_0 <- tst_with_prob[tst_with_prob$Class==0 & tst_with_prob$predclass==1,]
dim(tst_with_prob_0)
tst_with_prob_1 <- tst_with_prob_1[1,]
tst_with_prob_0 <- tst_with_prob_0[1,]

#find all the data which is similar and show that
k=5
simitms_1 <- order(gower.dist(tst_with_prob_1,tst_with_prob))[seq(1:k)]
#show 5 nearst point to the misclassified example
print(tst_with_prob[simitms_1,])

simitms_0 <- order(gower.dist(tst_with_prob_0,tst_with_prob))[seq(1:k)]
#show 5 nearst point to the misclassified example
print(tst_with_prob[simitms_0,])

#calculate accurayce and recall
ab <- table(tst_with_prob$predclass,tst_with_prob$Class)
print(ab)
recall <- ab[2,2]/(ab[1,2] + ab[2,2])
print(paste('recall',recall))
acc <- (ab[2,2] + ab[1,1])/(sum(ab))
print(paste('accuracy',acc))
print 
}
```

