Note:

Not all the edge cases are tested and thus there might be some issues and in case you find out about it. Let me know I will work to fix it.

Insome cases i have used a for loop where I could have used vectrize form and I did to make code more verbose as this code will be run for preprocessing and feature selection and wont be used in final production code. If you want to use this in final code some of this stuff can be used as it is and some parts may require optimization.

# I HAVE GIVEN LONG NAMES SO THAT I CAN LOOK AT METHOD AND KNOW WHAT IT IS.IN PROD CODE YOU WONT HAVE THIS MUCH LONG NAMES
# RATHER YOU CAN CUT THESE A BIT SORT :)

# DO NOT USE ANY OF THESE METHODS DIRECTLY AS IT EXPECT OUTPUTS TO BE IN CERTAIN FORMAT.

# Some of the code will run for really long for large data set or where the number of features are huge. Thus, becareful before running all parts and as suual some parts can be commentted where you see that it is taking huge time.


Now move towards what it does.

# Data preprocessing and modelling and mining is a tough task and it could be quite time consuming as well especially when your data is not so much separable and standard ML algo doesnt work. However, much tougher than this is getting the data and then preprocessing it.
The code in this folder are all related once you have a csv or any other format and you can put it in a data frame.

Then it takes from there and does quite a lot of stuff.
1. Pass_1.rmd. Put your csv file name here as well as specify any class/response variable. I

1.  Find the missing data.
2.	Find any features, which could be a candidate for factor columns.
3.	Find any factor columns, which have say, more than a given number of levels. This is useful because after some levels some models don’t work. Also, the graphs wouldn’t look nice. Also, the data might not be enough to represent all the levels. Thus some of the levels will have very few values.
4.	Find the correlation between columns.
5.	It also finds out the unusual data e.g. there might be some factor columns where one level is way too much underrepresented. We could combine these with other levels base don independence among levels wr.t. Class or remove these as these could cause issues especially when we will split the data in test and train data sets.
6.	It does a test for finding which columns are related to each other or independent of each other. This includes correlation for numeric columns and a chi square independence test for categorical variables. For relation between numeric and categorical variables I convert the numeric variable to categorical using chi merge and then chi-square of independence method is applied. This process doesn’t rely on the class column and entirely between features. Thus, it could be used on whole data set.

7.	Then it generates the graphs of individual columns as well as all combination of graphs. It could generate loads of graphs thus I always sample 20k rows and use these 20 k rows for graphs as it makes easy to graph the data on smaller set and at same time provides me enough information. I usually look for the trend especially where there is quite large distinct between given classes.



2. Pass_2. Once you have anlayzed the data and you want to fix some of it like removing the misisng data or removing some columns
and converting some columns to factors either as suggested by pass_1 or as needed as per your requirement.

The process is as follows.

1.  It checks the independence of all columns w.r.t. Class column. Continuous columns are converted to factor columns using a combination of unsupervised (equal frequency discretization) and supervised (Chi Merge discretization) process. This gave which columns are independent of the class. But this doesn’t mean that we can throw the columns away as they might be useful with interaction with another columns.
2.	Then, it ran the random forest model and ranked the top 20 features based on accuracy measure.
3.	Then, It ran the logistic regression by taking one column at a time. 
4.	Them, It ran the logistic regression but I used 2 columns and their interaction. I could have done the simple interaction as well. This was to find which column interaction should I use in my final LR.
5.	Then, It ran a decision tree on each column by using cp value of 0.001. This was to find which columns are discriminative.
6.	Them, it ran the decision tree but with 2 columns at a time. Again with same cp value.

3. This is where you would model based on your analysis from part 1 and your further deep analysis in pass 2.

Try any part3Modelling script and see what they do.


Enjoy coding........
